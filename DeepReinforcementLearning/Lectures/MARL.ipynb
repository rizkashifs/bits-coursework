{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SQTI91cGwBx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ed1a3e-2004-4c07-a989-83e960391ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 1 Q-table:\n",
            "[[[ 1.95246120e+03  1.41884879e+03  1.62694012e+03  1.55478139e+03]\n",
            "  [ 1.34950950e+03  5.30521190e+02  1.95178601e+03  9.82822992e+02]\n",
            "  [ 2.04452268e+02  4.54955729e+02  1.84204366e+03  4.97031141e+02]\n",
            "  [ 4.82951876e+02  2.10960065e+02  1.72921642e+03 -7.28912893e-02]\n",
            "  [-1.99500000e-01  9.13894514e+01  1.54196991e+03 -1.00000000e-01]]\n",
            "\n",
            " [[ 1.93956359e+03  0.00000000e+00  3.06352780e+02  4.54695487e+02]\n",
            "  [ 1.84926967e+03  8.55843442e+02  5.82511088e+02  4.57603066e+02]\n",
            "  [-1.90000000e-01 -2.08050000e-01  1.57022750e+03  1.37441885e+02]\n",
            "  [ 1.57701119e+03  3.49860946e+02  8.72039663e+01  1.06963035e+02]\n",
            "  [ 1.05092632e+03 -9.91039400e-02 -3.85243549e-01 -3.97009987e-01]]\n",
            "\n",
            " [[ 1.79251550e+03  1.51380408e+02  1.22337723e+02  2.23998049e+02]\n",
            "  [ 1.75159395e+03  5.86448106e+02  6.94469477e+02  4.54990837e+02]\n",
            "  [-2.80500000e-01  1.25725322e+03  5.21868049e+02  1.14292524e+02]\n",
            "  [ 1.39118636e+03 -4.04725896e-01  4.20655709e+02 -3.72286704e-01]\n",
            "  [ 7.54532187e+01 -6.07753241e-01  8.85268362e+02 -5.92549813e-01]]\n",
            "\n",
            " [[ 1.40866787e+03 -1.99500000e-01 -1.99500000e-01  2.65741112e+02]\n",
            "  [ 1.65738662e+03  2.60291276e+02  3.47666930e+02  5.74733830e+02]\n",
            "  [ 2.32494860e+02 -3.97009987e-01  1.56355027e+03  3.30736081e+02]\n",
            "  [ 2.40267313e+02  3.24769196e+02  1.46283352e+03  7.35196848e+01]\n",
            "  [ 1.01608359e+02  8.11562526e+02 -6.73192937e-01  1.68474700e+01]]\n",
            "\n",
            " [[-2.89050000e-01 -3.69645000e-01 -2.98502500e-01  1.03040382e+03]\n",
            "  [ 1.46030846e+03  1.23282078e+02  1.79085833e+02 -2.98502500e-01]\n",
            "  [ 1.07546706e+03 -4.95024938e-01 -4.06454568e-01 -4.30107680e-01]\n",
            "  [ 1.34327525e+03 -5.92549813e-01 -6.40460630e-01  1.07425580e+02]\n",
            "  [ 6.65092487e+01 -7.76920590e-01  1.16840943e+03  1.33516543e+02]]]\n",
            "Agent 2 Q-table:\n",
            "[[[-8.82208433e-01 -8.62123622e-01 -8.81932163e-01  4.17522103e+02]\n",
            "  [ 7.55189868e+01  6.44756038e+01 -7.50874786e-01  9.66734073e+02]\n",
            "  [ 5.42766170e-01  2.62747965e+02  1.28570782e+00  1.35330484e+03]\n",
            "  [ 2.09469972e+02  1.55168313e+03  2.41559716e+02  2.66949557e+02]\n",
            "  [-4.95024938e-01  1.55453753e+03  2.78941891e+02  1.80857063e+02]]\n",
            "\n",
            " [[-7.87363785e-01  9.84671071e-01 -6.89587064e-01  7.05853780e+02]\n",
            "  [-5.75193335e-01 -3.55826984e-01  5.57513907e+01  1.34660800e+03]\n",
            "  [ 3.46716660e+01  1.33016614e+02  2.14731288e+02  1.54405931e+03]\n",
            "  [ 4.57360080e+02  5.55706058e+02  1.23716140e+02  1.67393316e+03]\n",
            "  [ 5.14842424e+02  1.77318270e+03  6.89232097e+02  7.82526325e+02]]\n",
            "\n",
            " [[ 5.04293519e+01  4.70502241e+02  9.72876195e-01  2.94635738e+01]\n",
            "  [-5.04336358e-01 -4.74846488e-01  2.05489092e+01  1.06769114e+03]\n",
            "  [-4.53349597e-01 -3.07559809e-01  7.02865160e+01  1.56071533e+03]\n",
            "  [ 1.56542502e+02  1.02920907e+02  1.01503609e+02  1.75186556e+03]\n",
            "  [ 6.22300491e+02  1.87276432e+03  4.36624913e+02  7.62282613e+02]]\n",
            "\n",
            " [[ 3.79822108e+01 -5.62657214e-01  7.41476286e+01  1.13483246e+03]\n",
            "  [ 1.11137056e+01  2.51195967e+02  2.38888372e+01  1.50438318e+03]\n",
            "  [ 4.28924701e+02  3.15893075e+01  2.82406677e+02  1.69879422e+03]\n",
            "  [ 1.51766283e+02  2.92292944e+02  2.49319009e+01  1.85941239e+03]\n",
            "  [ 5.96693789e+02  1.97553967e+03  1.22134284e+03  8.31035910e+02]]\n",
            "\n",
            " [[-5.98032658e-01 -5.92549813e-01 -5.92549813e-01  7.77859014e+02]\n",
            "  [ 4.20325498e+00 -3.97009987e-01  1.43634953e+02  1.52661927e+03]\n",
            "  [-1.99500000e-01 -2.81402500e-01  1.39902875e+02  1.82616948e+03]\n",
            "  [ 4.77752664e+02  2.88171917e+02 -1.09500000e-01  1.96950580e+03]\n",
            "  [ 1.37541229e+03  1.67259979e+03  1.56375889e+03  1.97595708e+03]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment settings\n",
        "grid_size = 5\n",
        "goals = [(0, 0), (4, 4)]  # Goals for each agent\n",
        "n_actions = 4  # Up, Down, Left, Right\n",
        "n_states = grid_size * grid_size\n",
        "n_agents = 2\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "epsilon = 0.1\n",
        "episodes = 500\n",
        "\n",
        "# Initialize Q-tables for each agent\n",
        "Q = [np.zeros((n_states, n_actions)) for _ in range(n_agents)]\n",
        "\n",
        "# Convert state tuple to linear state index\n",
        "def state_to_index(state):\n",
        "    return state[0] * grid_size + state[1]\n",
        "\n",
        "# Action to state transition\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    if action == 0 and x > 0:  # Up\n",
        "        x -= 1\n",
        "    elif action == 1 and x < grid_size - 1:  # Down\n",
        "        x += 1\n",
        "    elif action == 2 and y > 0:  # Left\n",
        "        y -= 1\n",
        "    elif action == 3 and y < grid_size - 1:  # Right\n",
        "        y += 1\n",
        "    return (x, y)\n",
        "\n",
        "# Reward function considering goals and agent collisions\n",
        "def reward(state, next_state, goal):\n",
        "    if next_state == goal:\n",
        "        return 100  # Reward for reaching the goal\n",
        "    return -1  # Step penalty\n",
        "\n",
        "# Main training loop\n",
        "for episode in range(episodes):\n",
        "    # Initialize agents at random positions, making sure they're not on their goal\n",
        "    states = [(random.randint(0, grid_size-1), random.randint(0, grid_size-1)) for _ in range(n_agents)]\n",
        "    while states[0] == goals[0] or states[1] == goals[1]:\n",
        "        states = [(random.randint(0, grid_size-1), random.randint(0, grid_size-1)) for _ in range(n_agents)]\n",
        "\n",
        "    steps = 0\n",
        "    while states[0] != goals[0] or states[1] != goals[1]:\n",
        "        next_states = states.copy()\n",
        "        for i in range(n_agents):\n",
        "            state_index = state_to_index(states[i])\n",
        "\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, n_actions - 1)\n",
        "            else:\n",
        "                action = np.argmax(Q[i][state_index])\n",
        "\n",
        "            next_state = step(states[i], action)\n",
        "            next_states[i] = next_state\n",
        "\n",
        "            # Compute reward\n",
        "            r = reward(states[i], next_state, goals[i])\n",
        "\n",
        "            # Q-learning update\n",
        "            next_state_index = state_to_index(next_state)\n",
        "            Q[i][state_index, action] = Q[i][state_index, action] + \\\n",
        "                                        alpha * (r + gamma * np.max(Q[i][next_state_index]) - Q[i][state_index, action])\n",
        "\n",
        "        states = next_states\n",
        "        steps += 1\n",
        "        if steps > 100:  # Prevent infinite loops\n",
        "            break\n",
        "\n",
        "# Display learned Q-values\n",
        "for i in range(n_agents):\n",
        "    print(f\"Agent {i+1} Q-table:\")\n",
        "    print(Q[i].reshape((grid_size, grid_size, n_actions)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment settings\n",
        "grid_size = 5  # Define the size of the grid (5x5 in this case)\n",
        "goals = [(0, 0), (4, 4)]  # Define the goal positions for each of the two agents\n",
        "n_actions = 4  # Define the number of possible actions (Up, Down, Left, Right)\n",
        "n_states = grid_size * grid_size  # Calculate the total number of states in the grid\n",
        "n_agents = 2  # Define the number of agents\n",
        "\n",
        "# Hyperparameters for the learning process\n",
        "alpha = 0.1  # Learning rate, determines to what extent newly acquired information overrides old information\n",
        "gamma = 0.95  # Discount factor, represents the difference in importance between future rewards and present rewards\n",
        "epsilon = 0.1  # Exploration rate, the probability of choosing a random action instead of the best one\n",
        "episodes = 500  # Number of episodes to train the agents\n",
        "\n",
        "# Initialize Q-tables for each agent\n",
        "Q = [np.zeros((n_states, n_actions)) for _ in range(n_agents)]  # Create a zero-initialized Q-table for each agent\n",
        "\n",
        "# Function to convert a state tuple (x, y) into a linear index for the Q-table\n",
        "def state_to_index(state):\n",
        "    #print(state)\n",
        "    #print(state[0],grid_size,state[1])\n",
        "    return state[0] * grid_size + state[1]\n",
        "\n",
        "# Function to determine the new state after taking an action\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    # Update the state based on the action taken\n",
        "    if action == 0 and x > 0:  # Move up if not at the top edge\n",
        "        x -= 1\n",
        "    elif action == 1 and x < grid_size - 1:  # Move down if not at the bottom edge\n",
        "        x += 1\n",
        "    elif action == 2 and y > 0:  # Move left if not at the left edge\n",
        "        y -= 1\n",
        "    elif action == 3 and y < grid_size - 1:  # Move right if not at the right edge\n",
        "        y += 1\n",
        "    return (x, y)\n",
        "\n",
        "# Function to compute the reward for moving from one state to another\n",
        "def reward(state, next_state, goal):\n",
        "    if next_state == goal:\n",
        "        return 100  # Provide a high reward for reaching the goal\n",
        "    return -1  # Otherwise, return a step penalty\n",
        "\n",
        "# Main training loop\n",
        "for episode in range(1):\n",
        "    # Randomly initialize the positions of the agents, ensuring they are not on their goals\n",
        "    states = [(random.randint(0, grid_size-1), random.randint(0, grid_size-1)) for _ in range(n_agents)]\n",
        "    while states[0] == goals[0] or states[1] == goals[1]:\n",
        "        states = [(random.randint(0, grid_size-1), random.randint(0, grid_size-1)) for _ in range(n_agents)]\n",
        "\n",
        "    steps = 0  # Keep track of the number of steps taken\n",
        "    # Continue the episode until both agents reach their goals\n",
        "    while states[0] != goals[0] or states[1] != goals[1]:\n",
        "        next_states = states.copy()  # Prepare to update the states based on the actions taken\n",
        "        for i in range(n_agents):  # For each agent\n",
        "            state_index = state_to_index(states[i])  # Convert the state to an index\n",
        "\n",
        "            # Decide whether to take a random action or the best known action\n",
        "            if random.random() < epsilon:  # Exploration: choose a random action\n",
        "                action = random.randint(0, n_actions - 1)\n",
        "            else:  # Exploitation: choose the best action based on current Q-values\n",
        "                action = np.argmax(Q[i][state_index])\n",
        "\n",
        "            next_state = step(states[i], action)  # Determine the next state after taking the action\n",
        "            next_states[i] = next_state  # Update the next state for the agent\n",
        "\n",
        "            # Compute the reward for the action taken\n",
        "            r = reward(states[i], next_state, goals[i])\n",
        "\n",
        "            # Update the Q-value for the state-action pair using the Q-learning formula\n",
        "            next_state_index = state_to_index(next_state)\n",
        "            Q[i][state_index, action] = Q[i][state_index, action] + \\\n",
        "                                        alpha * (r + gamma * np.max(Q[i][next_state_index]) - Q[i][state_index, action])\n",
        "\n",
        "        states = next_states  # Update the states for the next iteration\n",
        "        steps += 1\n",
        "        if steps > 100:  # Break the loop if it takes too many steps, to prevent infinite loops\n",
        "            break\n",
        "\n",
        "# Display\n",
        "# Display learned Q-values\n",
        "for i in range(n_agents):\n",
        "    print(f\"Agent {i+1} Q-table:\")\n",
        "    # Reshape the Q-table for easier interpretation, then print it\n",
        "    # The Q-table is reshaped to (grid_size, grid_size, n_actions) for visualization\n",
        "    # This shows the Q-values for each action at each position in the grid\n",
        "    print(Q[i].reshape((grid_size, grid_size, n_actions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk3hRtf9CGE6",
        "outputId": "f27c18fd-4a95-4fb4-9f46-f300fe56d7e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 1 Q-table:\n",
            "[[[ 7.19781254e+02  4.10972948e+00  0.00000000e+00 -1.00000000e-01]\n",
            "  [-1.00000000e-01 -1.00000000e-01  7.27669141e+01  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 7.87043775e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
            "Agent 2 Q-table:\n",
            "[[[-0.1995  -0.1995  -0.1995  -0.1    ]\n",
            "  [-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1     -0.1     -0.1095   0.     ]\n",
            "  [ 0.       0.       0.       0.     ]]\n",
            "\n",
            " [[-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.19    -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1     -0.1     -0.1095   0.     ]\n",
            "  [ 0.       0.       0.       0.     ]]\n",
            "\n",
            " [[-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1995  -0.28905 -0.1     -0.1    ]\n",
            "  [-0.19    -0.19    -0.1995  -0.1    ]\n",
            "  [-0.1     -0.1     -0.1095   0.     ]\n",
            "  [ 0.       0.       0.       0.     ]]\n",
            "\n",
            " [[-0.1995  -0.1995  -0.1     -0.1    ]\n",
            "  [-0.1995  -0.19    -0.1995  -0.1    ]\n",
            "  [-0.1995  -0.1     -0.1     -0.1    ]\n",
            "  [-0.1     -0.1     -0.1095   0.     ]\n",
            "  [ 0.       0.       0.       0.     ]]\n",
            "\n",
            " [[-0.1995  -0.1     -0.1     -0.1    ]\n",
            "  [-0.1995  -0.1     -0.1     -0.1    ]\n",
            "  [-0.1     -0.1     -0.1095   0.     ]\n",
            "  [-0.1      0.       0.       0.     ]\n",
            "  [ 0.       0.       0.       0.     ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Q[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAuiVVZaGaGo",
        "outputId": "69cbfb62-fb1d-4e2f-fbba-d8bf78d38327"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    }
  ]
}